{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raimi/Playground/wall-segmentation/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import SAM\n",
    "from PIL import Image\n",
    "from package import pick_wall_point, distance_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./assets/PHOTO-2025-04-27-17-10-54.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "img_pil = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "pt = pick_wall_point(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/raimi/Playground/wall-segmentation/assets/PHOTO-2025-04-27-17-10-54.jpg: 1024x1024 1 0, 1313.0ms\n",
      "Speed: 6.3ms preprocess, 1313.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "sam = SAM(\"sam2_b.pt\")\n",
    "results = sam.predict(source=IMAGE_PATH, points=[pt], save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m farthest, closest = \u001b[43mdistance_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFarthest wall distance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfarthest\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m m\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClosest wall distance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclosest\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m m\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Playground/wall-segmentation/package/distance_estimation.py:33\u001b[39m, in \u001b[36mdistance_estimator\u001b[39m\u001b[34m(image_path, sam_results, depth_model)\u001b[39m\n\u001b[32m     30\u001b[39m     depth_np = depth_np.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 4) Mask and compute extremes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m wall_vals = \u001b[43mdepth_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwall_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wall_vals.size == \u001b[32m0\u001b[39m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo wall pixels detected.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "import torch\n",
    "\n",
    "# 1) Load your model once, as before\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model_configs = {\n",
    "    'vits': {'encoder':'vits','features':64, 'out_channels':[48,96,192,384]},\n",
    "    # ... your other configs ...\n",
    "}\n",
    "depth_anything = DepthAnythingV2(**model_configs[args.encoder])\n",
    "depth_anything.load_state_dict(torch.load(\n",
    "    f'checkpoints/depth_anything_v2_{args.encoder}.pth', map_location='cpu'))\n",
    "depth_anything = depth_anything.to(DEVICE).eval()\n",
    "\n",
    "# … later, inside your loop after you get `results` from SAM …\n",
    "farthest, closest = distance_estimator(\n",
    "    image_path=IMAGE_PATH,\n",
    "    sam_results=results,\n",
    "    depth_anything_model=depth_anything,\n",
    "    input_size=args.input_size\n",
    ")\n",
    "print(f\"Closest wall distance: {closest:.2f} m\")\n",
    "print(f\"Farthest wall distance: {farthest:.2f} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
