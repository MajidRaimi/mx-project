# ğŸ§± Wall Segmentation Pipeline

A powerful computer vision system that combines AI-powered wall segmentation with drone integration for automated wall analysis and path planning. This project enables real-time wall detection, segmentation, and navigation path generation using state-of-the-art deep learning models.

## âœ¨ Key Features

ğŸ¤– **AI-Powered Wall Detection** - Uses Segformer for semantic segmentation and SAM2 for precise wall boundaries  
ğŸš **Drone Integration** - Full DJI Tello drone support with real-time image capture  
ğŸ“Š **Interactive Web Interface** - Streamlit-based GUI with step-by-step processing visualization  
ğŸ¯ **Smart Point Selection** - Automatically identifies optimal wall points for analysis  
ğŸ—ºï¸ **Path Planning** - Generates navigation grids and connection paths for autonomous movement  
ğŸ”§ **Flexible Processing** - Supports both uploaded images and live drone capture  

## ğŸš€ Quick Start

### Prerequisites

- Python 3.13+
- CUDA-compatible GPU (recommended)
- DJI Tello drone (optional)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd wall-segmentation
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download SAM2 model**
   - Place `sam2_t.pt` in the project root directory

4. **Run the application**
   ```bash
   streamlit run app.py
   ```

### ğŸ³ Docker Setup

```bash
# Build the container
docker build -t wall-segmentation .

# Run the application
docker run -p 8501:8501 wall-segmentation
```

Access the application at `http://localhost:8501`

## ğŸ® How to Use

### ğŸ“· Image Upload Mode
1. Upload an image using the file uploader
2. Adjust the GAP slider (30-100) for grid spacing
3. Watch the 4-step processing pipeline:
   - **Step 1**: Wall point detection
   - **Step 2**: SAM segmentation  
   - **Step 3**: Grid point generation
   - **Step 4**: Path connection

### ğŸš Drone Mode
1. Connect your DJI Tello drone
2. Monitor battery status and flight state
3. Configure distance and height parameters
4. Use drone controls:
   - **Take Off** ğŸš - Launch the drone
   - **Capture Image** ğŸ“¸ - Position and capture wall image
   - **Land** ğŸ›¬ - Safely land the drone

## ğŸ—ï¸ Architecture

### Core Components

```
ğŸ“ package/
â”œâ”€â”€ pick_wall_point.py      # AI-powered wall detection
â”œâ”€â”€ draw_result_on_image.py # Segmentation visualization
â”œâ”€â”€ draw_points.py          # Grid generation
â”œâ”€â”€ connect_points.py       # Path planning
â”œâ”€â”€ distance_estimation.py  # Depth analysis
â””â”€â”€ save_image.py          # Image utilities

ğŸ“ services/
â””â”€â”€ drone.py               # DJI Tello integration

ğŸ“ data/
â””â”€â”€ commands.csv           # Drone command sequences
```

### Processing Pipeline

```mermaid
graph LR
    A[Input Image] --> B[Wall Detection]
    B --> C[SAM Segmentation]
    C --> D[Grid Generation]
    D --> E[Path Planning]
    E --> F[Output Visualization]
```

## ğŸ”§ Configuration

### Grid Spacing
- **GAP Parameter**: Controls the density of navigation points (30-100 pixels)
- Lower values = more detailed paths
- Higher values = faster processing

### Drone Settings
- **Distance from Wall**: 0.5-10 meters
- **Flight Height**: 1-5 meters
- **Battery Monitoring**: Automatic safety checks

## ğŸ“Š Output Examples

The system generates a complete visualization sequence:

1. **ğŸ–¼ï¸ Black & White Wall Mask** - Binary segmentation result
2. **ğŸ¯ Selected Wall Point** - Optimal analysis point highlighted
3. **ğŸ“ Wall Segmentation** - Precise boundary detection
4. **ğŸ§® Grid Points** - Navigation waypoints overlay
5. **ğŸ›£ï¸ Final Path** - Connected route for autonomous navigation

## ğŸ› ï¸ Technical Stack

- **Deep Learning**: PyTorch, Ultralytics SAM, Transformers
- **Computer Vision**: OpenCV, PIL, scikit-image
- **Web Interface**: Streamlit
- **Drone Control**: djitellopy
- **Scientific Computing**: NumPy, SciPy
- **Visualization**: Matplotlib, Seaborn

## ğŸ“ Project Structure

```
wall-segmentation/
â”œâ”€â”€ ğŸ“„ app.py                 # Main Streamlit application
â”œâ”€â”€ ğŸ“„ requirements.txt       # Python dependencies
â”œâ”€â”€ ğŸ“„ Dockerfile            # Container configuration
â”œâ”€â”€ ğŸ“ package/              # Core processing modules
â”œâ”€â”€ ğŸ“ services/             # Drone integration
â”œâ”€â”€ ğŸ“ helpers/              # Utility functions
â”œâ”€â”€ ğŸ“ sample_input/         # Test images
â”œâ”€â”€ ğŸ“ demo/                 # Example outputs
â”œâ”€â”€ ğŸ“ assets/               # Demo images
â””â”€â”€ ğŸ“ output/               # Generated results
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Ultralytics** for the SAM2 implementation
- **Hugging Face** for the Segformer models
- **DJI** for the Tello drone SDK
- **Streamlit** for the amazing web framework

---

**Built with â¤ï¸ for autonomous navigation and computer vision research**